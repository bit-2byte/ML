# -*- coding: utf-8 -*-
"""Zomato Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XIBfPkxssBrYjfeUcBFt4sIzhvXtaGkQ
"""

from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.metrics import silhouette_samples, silhouette_score
pd.set_option('display.max_columns',None)

from google.colab import drive
drive.mount('/content/drive')

df1 = pd.read_csv('/content/drive/MyDrive/data/Zomato Restaurant names and Metadata.csv')
df2 = pd.read_csv('/content/drive/MyDrive/data/Zomato Restaurant reviews.csv')

"""# EXPLORING THE DATASET"""

df1.head()

df1.tail()

df2.head()

df2.tail()

df1.size

df2.size

"""# Cheking null value in data set"""

df1.isna().sum()

df2.isna().sum()

df1.describe(include = 'all')

df2.describe(include ='all')

df1.info()

df2.info()

"""# Exploratory Data Analysis
**bold text**
"""

df1.head()

cost_transform={
    '800':800,
    '1200' :1200,
    '1500' : 1500,
    '500' : 500,
    '300' : 300,
    '1000': 1000,
    '350' : 350,
    '400' : 400,
    '1,600' :1600,
    '750' : 750,
    '550' : 550,
    '1,900' : 1900,
    '450' : 450,
    '150' : 150,
    '1,400' : 1400,
    '1,100' : 1100,
    '600' : 600,
    '200' : 200,
    '900' : 900,
    '700' : 700,
    '1,700' : 1700,
    '2,500' : 2500,
    '850' : 850,
    '650' : 650,
    '1,800' : 1800,
    '2,800' : 2800,
    '1,750' : 1750,
    '250' : 250

}
df1['cost']=df1.Cost.map(cost_transform)
df1.drop('Cost',axis=1,inplace=True)

#the below graph shows the distribution of cost column
fig=plt.figure(figsize=(15,8))
ax=fig.gca()
sns.histplot(df1.cost)

fig=plt.figure(figsize=(15,8))
ax=fig.gca()
sns.histplot(df1.Collections)
plt.xticks(rotation=90)

#the graph shows the cost and collection relation
fig=plt.figure(figsize=(15,8))
ax=fig.gca()
df=df1.groupby(['Collections'])['cost'].sum().reset_index()
plt.bar(df['Collections'],df['cost'])
plt.xticks(rotation=90)

df=df1.groupby(['Cuisines'])['cost'].count().reset_index()
fig=plt.figure(figsize=(15,8))
ax=fig.gca()
plt.bar(df['Cuisines'],df['cost'])
plt.xticks(rotation=90)

df= df1.groupby(['Name'])['cost'].sum().reset_index()
fig=plt.figure(figsize=(15,8))
ax=fig.gca()
plt.bar(df['Name'],df['cost'])
plt.xticks(rotation=90)

df = df1.groupby(['Timings'])['cost'].count().reset_index()
fig=plt.figure(figsize=(15,8))
ax= fig.gca()
plt.bar(df['Timings'],df['cost'])
plt.xticks(rotation=90)

"""# Exploring Dataset2"""

rating_trans={
    '5':5,
    '4.5':4.5,
    '4':4,
    '3.5':3.5,
    '3':3,
    '2.5':2.5,
    '2':2,
    '1.5':1.5,
    '1':1,
    'Like':3,
    'nan':1
}
df2['Rateing']=df2.Rating.map(rating_trans)
df2.drop('Rating',inplace=True,axis=1)

fig=plt.figure(figsize=(15,8))
ax=fig.gca()
sns.distplot(df2.Rateing)

#The graph shows the relation between restaurant and rating
df=df2.groupby(['Restaurant'])['Rateing'].count().reset_index()
fig=plt.figure(figsize=(15,8))
ax=fig.gca()
plt.bar(df['Restaurant'],df['Rateing'])
plt.xticks(rotation=90)

#The below graph show the distribution of the reviewer
fig = plt.figure(figsize=(15,8))
ax = fig.gca()
sns.histplot(df2.Reviewer)
plt.xticks(rotation=90)

# Below chart shows the top 10 reviewer
d1 = dict(df2.Reviewer.value_counts())
d1 = dict(sorted(d1.items(),key=lambda item: item[1],reverse=True))
a= list(d1.keys())
b= list(d1.values())
plt.pie(b[:10],labels=a[:10], startangle=0)

fig=plt.figure(figsize=(25,8))
ax = fig.gca()
sns.histplot(df2.Metadata)
plt.xticks(rotation=90)

#The below pie chart shows the top 10 Metadata
d1=dict(df2.Metadata.value_counts())
d1=dict(sorted(d1.items(),key=lambda item: item[1], reverse=True))
a=list(d1.keys())
b=list(d1.values())
plt.pie(b[:10],labels=a[:10],startangle=0)

"""# Feature Engineering"""

df1.isna().sum()

df2.isna().sum()

df2.dropna(inplace=True,axis=0)

#below function will help to split the string using the delemeter "," and return the list of strings.
def cuisines_unique(a):
  crusin_unique=[]
  for i in a:
    cstr=''
    for j in i:
      if j==',':
        crusin_unique.append(cstr)
        cstr=''
      else:
        cstr=cstr+j
    return set(crusin_unique)

def strp_first_space(x):
  cc=[]
  for i in x:
    if i[0]=='':
      cc.append(i[1:])
    else:
      cc.append(i)
  return cc

# Creting individual column for each cuisines from all the cuisines available in the string
a=df1['Cuisines'].unique()
b=cuisines_unique(a)
c=list(b)
cc=strp_first_space(c)

for i in cc:
  df1[i]=df1.Cuisines.apply(lambda x:1 if i in x else 0)

# Creating a individual columns for each collections from the all the collection available in the string
a=df1['Collections'].unique()
final_list=[]
a=list(a)
for i in a:
  i=str(i)
  l1=i.split(',')
  for j in l1:
    final_list.append(j)
c1=strp_first_space(final_list)

for i in c1:
  if i!= None or i!='Nan':
    df1[i]=df1.Collections.apply(lambda x:1 if i in str(x) else 0 if str(x)!='Nan' else 0)

df1.isna().sum()

df1.head()

"""# Feature Engineering for Dataset2"""

# Since same information is available in both table with different column name. We'll make it with a sae name
#this will help us to perform joint operation if needed
df2['name']=df2['Restaurant']
df2.drop(['Restaurant'], inplace=True, axis=1)

# The below function will use to make the Reviews column
def review_collector(x):
  if 'Review' in x and 'Follower' in x:
    l1=x.split(',')
    for i in l1:
      if "Reviews" in i:
        return int(i.strip('Reviews'))
      elif 'Review' in i:
        return int(i.strip('Review'))
      else:
        continue


  elif 'Reviews' in x and 'Follower' not in x:
    return int(x.strip('Reviews'))

  elif 'Review' in x and 'Follower' not in x:
    return int(x.strip('Review'))

  else:
    return 0

## The below function will use to make the Follower column
def follower_collector(x):
  if 'Review' in x and 'Follower' in x:
    l1= x.split(',')
    for i in l1:
      if 'Followers' in i:
        try:
          return int(i.strip('Followers').strip(' '))
        except:
          return 0
      elif 'Follower' in i:
        try:
          return int(i.strip('Follower').strip(' '))
        except:
          return 0
      elif "'Followers" in i:
        try:
          return int(i.strip('Followers').strip(' '))
        except :
          return 0
      elif 'Follower' in i:
        try:
          return int(i.strip('Follower').strip(' '))
        except:
          return 0
      else:
        continue

  elif 'Followers' in x and 'Review' not in x:
    try:
      return int(x.strip('Followers').strip(' '))
    except:
      return 0
  elif 'Followers' in x and 'Reviews' not in x:
    try:
      return int(x.strip('Followers').strip(' '))
    except:
      return 0
  elif 'Follower' in x and 'Review' not in x:
    try:
      return int(x.strip('Follower').strip(' '))
    except:
      return 0

  elif 'Follower' in x and 'Review' not in x:
    try:
      return int(x.strip('Follower').strip(' '))
    except:
      return 0
  else:
    return 0

# Creating new Reviews column using above function.
df2['Reviews']=df2.Metadata.apply(review_collector)

df2['Followers']=df2.Metadata.apply(follower_collector)

df2[df2['Reviewer']=='Abhishek Mahajan']

df2

df2.isna().count()

df2.isna().sum()

# Creating a dictionary as Name_weight based on the restaurant name and it's review frequency
Review_weight=dict(df2.Reviewer.value_counts())
Name_weight=dict(df2.name.value_counts())

# Assigning the weight based on the frequency of review of the restaurant
df2['Name_weight']=df2.name.map(Name_weight)

df2['Review_weight']=df2.Reviewer.map(Review_weight)

"""# **Creating dataset 3 :-**  
**The dataset 3 will contain the copy of dataset 1 and merging sum and mean of few attribute of dataset 2**
"""

df3=df1.copy()
df3.head()